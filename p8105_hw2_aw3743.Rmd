---
title: "P8105 Homework 2"
author: Aisha Waggeh
date: October 1 2025
output: github_document
---

```{r setup}
library(tidyverse)
library(readxl)
```


## Problem 1

This problem uses 3 FiveThrityEight datasets:

- `pols-month.csv` – counts of national politicians (governors, senators, representatives) by party, and whether the sitting president is Republican or Democrat.  
- `snp.csv` – monthly closing values of the S&P stock market index.  
- `unemployment.csv` – monthly unemployment rates from the US Bureau of Labor Statistics.  

Our goal is to clean and merge these datasets using **year** and **month** as keys.

*Read the pols data*

```{r}
pols_df = read_csv("./data/pols-month.csv")
pols_df = janitor::clean_names(pols_df)
```

*Clean the pols data + formart*

```{r}
# Clean the pols data
pols_clean = pols_df |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    month = month.name[month],
    day = as.integer(day),
    president = ifelse(prez_gop == 1, "gop", "dem")
  ) |>
select( -prez_dem, -prez_gop, -day) |>
 arrange(year, month)  

pols_clean
```

*Read the snp data*

```{r}
snp_df = read_csv("./data/snp.csv")
snp_df = janitor::clean_names(snp_df)
```

*Clean the snp data + formart*

```{r}
snp_clean = snp_df |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(
    year = as.integer(year),
    # Convert 2-digit year to 4-digit year:
    # Years 50-99 -> 1950-1999, Years 00-49 -> 2000- onwards
    year = ifelse(year >= 50, 1900 + year, 2000 + year),
    month = as.integer(month),
    month = month.name[month],
    day = as.integer(day)
  ) |>
  select(-day) |> # Removing day column since we are merging by month only
  arrange(year, month) |>
  select(year, month, close)

snp_clean
```

*Read the unemployment data*

```{r}
unemployment_df = read_csv("./data/unemployment.csv")
unemployment_df = janitor::clean_names(unemployment_df)
```

*Tidy the unemployment data + formart*

```{r}
unemployment_clean = unemployment_df |>
  pivot_longer(
    cols = jan:dec,
    names_to = "month", 
    values_to = "unemployment_rate"
  ) |>
  mutate(
    month = str_to_title(month),  # Convert to Title Case
    month = month.name[match(month, month.abb)],  # Match with full month names
    year = as.integer(year)
  ) |>
  select(year, month, unemployment_rate) |>
  arrange(year, month)

unemployment_clean
```


*Merge all (pols + snp + unemployment) datasets*

```{r}
#Need to make sure that we use consistent data type for my merge
pols_clean_fixed = pols_clean |>
  mutate(month = as.character(month))

snp_clean_fixed = snp_clean |>
  mutate(month = as.character(month))

unemployment_clean_fixed = unemployment_clean |>
  mutate(month = as.character(month))
```


```{r}
# Now ready for merge, will merge snp into pols first
merged_data = 
  left_join(pols_clean, snp_clean, by = c("year", "month"))

 # Then merge unemployment into the result
final_data =
  left_join(merged_data, unemployment_clean, by = c("year", "month"))

final_data
```



```{r}
# Need to check the actual start years for each dataset
print("Pols data start:")
pols_clean |> summarize(min_year = min(year), max_year = max(year))

print("SNP data start:")
snp_clean |> summarize(min_year = min(year), max_year = max(year))

print("Unemployment data start:")
unemployment_clean |> summarize(min_year = min(year), max_year = max(year))

# Checking when SNP data actually begins
snp_clean |> arrange(year) |> head(10)
```

*    The pols_clean dataset contains 822 monthly observations of US political party control from 1947 to 2015. It tracks the number of Republican and Democratic politicians across governors, senators, and representatives, along with the sitting president's party. Each row represents the political power distribution for a specific month, revealing the political landscape over nearly seven decades.

*    The snp dataset contains 787 monthly observations of S&P 500 stock market closing values from 1950 to 2015. It tracks the closing price of the S&P 500 index for each month, showing the performance of the US stock market over six decades. The data reveals long-term trends.

*    The unemployment dataset contains 816 monthly observations of US unemployment rates from 1948 to 2015. It tracks the monthly unemployment percentage, providing a measure of economic health and labor market conditions over nearly seven decades. The data shows fluctuations in employment levels across different economic cycles and historical periods.

*    Finally, the merged dataset combines political, financial, and economic indicators by joining the three datasets using year and month as keys. NA values appear for the unemployment rate in 1947 and for S&P closing values from 1947-1949 because these datasets start later than the political data (unemployment in 1948, S&P in 1950). The resulting dataset contains 822 monthly observations from 1947 to 2015, enabling analysis of how political changes correlate with economic trends across decades. This comprehensive resource allows us to examine relationships between government leadership, stock market performance, and labor market conditions over time.


## Problem 2

This problem uses the Mr. Trash Wheel dataset, to read & clean.

```{r}
mr_trash_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                        sheet = "Mr. Trash Wheel",
                        range = "A2:N655",
                        col_types = "text")

mr_trash_df = janitor::clean_names(mr_trash_df)
```


*Clean the Mr. Trash Wheel data*

```{r}
# Clean the data
mr_trash_clean = mr_trash_df |>
  filter(!is.na(dumpster) & !is.na(weight_tons)) |>
  mutate(
    across(c(dumpster, year, weight_tons, volume_cubic_yards), as.numeric),
    across(c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, 
             plastic_bags, wrappers, sports_balls, homes_powered),  # Now it should be homes_powered
           ~ as.numeric(gsub(",", "", .))),
    date = as.Date(as.numeric(date), origin = "1899-12-30"),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr. Trash Wheel"
  ) |>
  arrange(year, month, dumpster)

mr_trash_clean
```

*Read the Professor Trash Wheel data*
```{r}
# Read Professor Trash Wheel data with manual column names
prof_trash_df = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                          sheet = "Professor Trash Wheel", 
                          range = "A1:M118",
                          col_names = c("dumpster", "month", "year", "date", "weight_tons",
                                       "volume_cubic_yards", "plastic_bottles", "polystyrene",
                                       "cigarette_butts", "glass_bottles", "plastic_bags", 
                                       "wrappers", "homes_powered"),
                          col_types = "text")

# Remove header row and clean names
prof_trash_df = prof_trash_df |>
  slice(-1) |>
  janitor::clean_names()

print("Cleaned column names:")
names(prof_trash_df)

```

*Clean the Professor Trash Wheel data + format*

```{r}
# Clean the data using the correct column names
prof_trash_clean = prof_trash_df |>
  filter(!is.na(dumpster) & !is.na(weight_tons)) |>
  mutate(
    across(c(dumpster, year, weight_tons, volume_cubic_yards), as.numeric),
    across(c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, 
             plastic_bags, wrappers, homes_powered),
           ~ as.numeric(gsub(",", "", .))),
    date = as.Date(as.numeric(date), origin = "1899-12-30"),
    trash_wheel = "Professor Trash Wheel"
  ) |>
  arrange(year, month, dumpster)

prof_trash_clean

```

*Read the Gwynnda Trash Wheel data*

```{r}
# Read with correct column names for Gwynnda's actual structure since before x1..... xetc
gwynnda_trash_df_raw = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                                 sheet = "Gwynnda Trash Wheel",
                                 range = "A1:L265",
                                 col_names = c("dumpster", "month", "year", "date", "weight_tons",
                                              "volume_cubic_yards", "plastic_bottles", "polystyrene",
                                              "cigarette_butts", "plastic_bags", "wrappers", 
                                              "homes_powered"),
                                 col_types = "text")

print("Column names:")
print(names(gwynnda_trash_df_raw))

# Now process
gwynnda_trash_df = gwynnda_trash_df_raw |>
  slice(-1) |>  # Remove the header row
  janitor::clean_names()

print("Cleaned column names:")
print(names(gwynnda_trash_df))


```

*Clean the Gwynnda*

```{r}
# Clean the data
gwynnda_trash_clean = gwynnda_trash_df |>
  filter(!is.na(dumpster) & !is.na(weight_tons)) |>
  mutate(
    # Convert all numeric columns with better handling
    across(c(dumpster, year, weight_tons, volume_cubic_yards, 
             plastic_bottles, polystyrene, cigarette_butts,
             plastic_bags, wrappers, homes_powered),
           ~ {
             # Remove commas and convert to numeric, handling empty strings
             clean_val = gsub(",", "", .)
             # Convert empty strings to NA before numeric conversion
             clean_val[clean_val == ""] <- NA
             as.numeric(clean_val)
           }),
    date = as.Date(as.numeric(date), origin = "1899-12-30"),
    trash_wheel = "Gwynnda Trash Wheel"
  ) |>
  arrange(year, month, dumpster)

gwynnda_trash_clean
```

*Now we can combine all the 3 data sets*

```{r}
all_trash_wheels = bind_rows(mr_trash_clean, prof_trash_clean, gwynnda_trash_clean) |>
  arrange(year, month, dumpster)

all_trash_wheels
```

*Answering the specific questions for Problem 2*
```{r}
# Total weight of trash collected by Professor Trash Wheel
prof_total_weight = all_trash_wheels |>
  filter(trash_wheel == "Professor Trash Wheel") |>
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

# Total cigarette butts collected by Gwynnda in June 2022
gwynnda_june_2022 = all_trash_wheels |>
  filter(trash_wheel == "Gwynnda Trash Wheel",
         year == 2022,
         month == "June") |>
  summarize(total_cigarettes = sum(cigarette_butts, na.rm = TRUE))

prof_total_weight
gwynnda_june_2022
```


*Dataset Summary*

```{r}
# Check the actual column names in the combined dataset
print("Column names in all_trash_wheels:")
names(all_trash_wheels)

# Check if any column names are empty
print("Any empty column names?")
any(names(all_trash_wheels) == "")

# Check the structure of the dataset
print("Dataset structure:")
glimpse(all_trash_wheels)

#Now we can do a dataset summary to see what is happening 
trash_summary = tibble(
  n_rows = nrow(all_trash_wheels),
  n_cols = ncol(all_trash_wheels),
  min_year = min(all_trash_wheels$year, na.rm = TRUE),
  max_year = max(all_trash_wheels$year, na.rm = TRUE),
  key_variables = paste(names(all_trash_wheels), collapse = ", ")
)

# Check Professor Trash Wheel total weight
prof_total_weight = all_trash_wheels |>
  filter(trash_wheel == "Professor Trash Wheel") |>
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

prof_total_weight

trash_summary
```

*    The combined Trash Wheel dataset contains 1,031 observations and 15 columns, covering the period from 2014 to 2024. The dataset tracks trash collection from three water-cleaning devices: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. Key variables include dumpster number, month, year, weight in tons, volume in cubic yards, and counts of specific trash types like plastic bottles, cigarette butts, and sports balls. Professor Trash Wheel collected a total of 243.4 tons of trash, while Gwynnda collected 18,120 cigarette butts in June 2022. 



## Problem 3


*Read and clean the ZORI Dataset*
```{r}

# Read ZORI data with manual column name handling
zori_df = read_csv("./data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", 
                  col_names = TRUE,
                  show_col_types = FALSE)

# Manually assign clean names to first 9 columns
clean_names_first9 <- c("region_id", "size_rank", "region_name", "region_type", 
                       "state_name", "state", "city", "metro", "county_name")
names(zori_df)[1:9] <- clean_names_first9

# Pivot longer and convert date columns
zori_clean = zori_df |>
  pivot_longer(
    cols = 10:ncol(zori_df),  # Date columns
    names_to = "date_str",
    values_to = "rental_price"
  ) |>
  mutate(
    # date_str is already in "YYYY-MM-DD" format, use it directly
    date = as.Date(date_str),
    year = year(date),
    month = month(date, label = TRUE),
    month_num = month(date)
  ) |>
  filter(!is.na(rental_price)) |>
  rename(zip_code = region_name) |>
  arrange(zip_code, date)

zori_clean
```


*Read and clean the ZIP Codes data set*

```{r}
# Read ZIP code data
zip_df = read_csv("./data/Zip Codes.csv")
zip_df = janitor::clean_names(zip_df)

# Clean ZIP code data
zip_clean = zip_df |>
  mutate(
    zip_code = as.character(zip_code),
    borough = county
  ) |>
  select(zip_code, borough, neighborhood) |>
  filter(!is.na(zip_code)) |>
  distinct()

zip_clean

```

*Now merge the 2 datasets*

```{r}
# I kept getting an error for the zip-code file
# Converting zip_code to character and overwrite the my data set
zori_clean = zori_clean |>
  mutate(zip_code = as.character(zip_code))

# Verify the conversion worked
print("ZORI zip_code type after conversion:")
class(zori_clean$zip_code)

print("ZORI zip_code sample after conversion:")
head(zori_clean$zip_code)

# Now merge the datasets
final_rental_data = zori_clean |>
  left_join(zip_clean, by = "zip_code")

final_rental_data

# Check the column names in the merged dataset
print("Column names in final_rental_data:")
names(final_rental_data)

# Check if any column names are empty
print("Empty column names:")
any(names(final_rental_data) == "")

# Check the structure
print("Dataset structure:")
glimpse(final_rental_data)

# Create summary using the direct approach
total_obs = nrow(final_rental_data)
unique_zips = n_distinct(final_rental_data$zip_code)
unique_neighborhoods = n_distinct(final_rental_data$neighborhood)
min_date = min(final_rental_data$date)
max_date = max(final_rental_data$date)

dataset_summary = tibble(
  total_observations = total_obs,
  unique_zip_codes = unique_zips,
  unique_neighborhoods = unique_neighborhoods,
  date_range = paste(min_date, "to", max_date)
)

dataset_summary
```


*Identify Zip Codes missing from ZORI data*

```{r}
# ZIP codes in ZIP dataset but not in ZORI
missing_zips = zip_clean |>
  anti_join(zori_clean, by = "zip_code")

missing_zips_count = nrow(missing_zips)

print(paste("ZIP codes missing from ZORI data:", missing_zips_count))

# Count by borough
missing_by_borough = missing_zips |>
  count(borough, name = "missing_count") |>
  arrange(desc(missing_count))

print("Missing ZIP codes by borough:")
missing_by_borough

# Show sample of missing ZIP codes
missing_sample = missing_zips |>
  head(10)

print("Sample of missing ZIP codes:")
missing_sample

# Summary of missing ZIP codes by borough
missing_summary = missing_zips |>
  group_by(borough) |>
  summarize(missing_count = n()) |>
  arrange(desc(missing_count))

missing_summary

# Get the list of boroughs with missing ZIP codes
missing_boroughs = missing_summary$borough
```

*COVID-19 rental price comparison*

```{r}
# Only following approach works on my end
jan_2020 = final_rental_data |>
  filter(month_num == 1, year == 2020) |>
  select(zip_code, borough, neighborhood, rental_price_2020 = rental_price)

jan_2021 = final_rental_data |>
  filter(month_num == 1, year == 2021) |>
  select(zip_code, rental_price_2021 = rental_price)

# Join and calculate changes
covid_comparison = jan_2020 |>
  inner_join(jan_2021, by = "zip_code") |>
  mutate(
    price_change = rental_price_2021 - rental_price_2020,
    percent_change = (rental_price_2021 - rental_price_2020) / rental_price_2020 * 100
  )

# Top 10 largest price drops
top_drops = covid_comparison |>
  arrange(price_change) |>
  head(10)

print("Top 10 largest price drops (Jan 2020 to Jan 2021):")
top_drops

# Get the range of percentage drops
min_pct_drop = round(min(top_drops$percent_change), 1)
max_pct_drop = round(max(top_drops$percent_change), 1)

# Got boroughs represented in top drops
top_drop_boroughs = unique(top_drops$borough)

print(paste("Percentage drop range:", min_pct_drop, "% to", max_pct_drop, "%"))
print(paste("Boroughs with largest drops:", paste(top_drop_boroughs, collapse = ", ")))

# Show the complete top 10 drops
top_drops_complete = covid_comparison |>
  arrange(price_change) |>
  head(10)

print("Complete top 10 largest price drops:")
top_drops_complete

# Get the range of percentage drops
min_pct_drop = round(min(top_drops_complete$percent_change), 1)
max_pct_drop = round(max(top_drops_complete$percent_change), 1)

# Get boroughs represented in top drops
top_drop_boroughs = unique(top_drops_complete$borough)

print(paste("Percentage drop range:", min_pct_drop, "% to", max_pct_drop, "%"))
print(paste("Boroughs with largest drops:", paste(top_drop_boroughs, collapse = ", ")))

```


*Answer to Problem 3*

*    The combined rental dataset contains 10,677 observations tracking prices across 149 ZIP codes and 43 NYC neighborhoods from 2015-2024. Analysis reveals 171 ZIP codes are missing from the rental data, primarily from Manhattan (102) and Queens (44), likely representing commercial or low-rental areas. During COVID-19, the largest price drops ranged from 14.4% to 22.4%, all concentrated in Manhattan neighborhoods. Lower Manhattan, Chelsea, and the Lower East Side experienced the most severe declines due to pandemic-related migration from dense urban centers. The dataset captures both the initial pandemic shock and subsequent recovery through 2024.